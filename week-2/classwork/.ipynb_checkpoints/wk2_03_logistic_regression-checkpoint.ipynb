{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AGENDA\n",
    "1. Refresh your memory on how to do linear regression in scikit-learn\n",
    "2. Attempt to use linear regression for classification\n",
    "3. Show you why logistic regression is a better alternative for classification\n",
    "4. Brief overview of probability, odds, e, log, and log-odds\n",
    "5. Explain the form of logistic regression\n",
    "6. Compare logistic regression with other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1: PREDICTING A CATEGORICAL RESPONSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','glass_type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass['assorted'] = glass.glass_type.map({1:0, 2:0, 3:0, 4:0, 5:1, 6:1, 7:1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: USING LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression can do what we just did:\n",
    "# fit a linear regression model and store the class predictions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e9)\n",
    "feature_cols = ['al']\n",
    "X = glass[feature_cols]\n",
    "y = glass.assorted\n",
    "logreg.fit(X, y)\n",
    "assorted_pred_class = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print the class predictions\n",
    "assorted_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add predicted class to DataFrame\n",
    "glass['assorted_pred_class'] = assorted_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort DataFrame by al\n",
    "glass.sort_values('al', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the class predictions again\n",
    "plt.scatter(glass.al, glass.assorted)\n",
    "plt.plot(glass.al, glass.assorted_pred_class, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we wanted the **predicted probabilities** instead of just the **class predictions**, to understand how confident we are in a given prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "logreg.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# store the predicted probabilites of class 1\n",
    "assorted_pred_prob = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assorted_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass['assorted_pred_prob'] = sorted(assorted_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the predicted probabilities\n",
    "plt.scatter(glass.al, glass.assorted)\n",
    "plt.plot(glass.al, glass.assorted_pred_prob, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# examine some example predictions\n",
    "print (logreg.predict_proba([[1]]))\n",
    "print (logreg.predict_proba([[2]]))\n",
    "print (logreg.predict_proba([[3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is this? \n",
    "* The first column indicates the predicted probability of **class 0**, and the second column indicates the predicted probability of **class 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3: PROBABILITY, ODDS, e, LOG, LOG-ODDS\n",
    "\n",
    "\n",
    "\n",
    "## LOGISTIC REGRESSION ANALYSIS: UNDERSTANDING ODDS AND PROBABILITY\n",
    "\n",
    "Probability and odds measure the same thing: **the likelihood of a specific outcome**.\n",
    "\n",
    "People use the terms odds and probability interchangeably in casual usage, but that is unfortunate. It just creates confusion because they are not equivalent.\n",
    "\n",
    "They measure the same thing on different scales. Imagine how confusing it would be if people used degrees Celsius and degrees Fahrenheit interchangeably. “It’s going to be 35 degrees today” could really make you dress the wrong way.\n",
    "\n",
    "In measuring the likelihood of any outcome, we need to know two things: how many times something happened and how many times it could have happened, or equivalently, how many times it didn’t. The outcome of interest is called a success, whether it’s a good outcome or not.\n",
    "\n",
    "The other outcome is a failure. Each time one of the outcomes could occur is called a trial. Since each trial must end in success or failure, number of successes and number of failures adds up to total number of trials.\n",
    "\n",
    "Probability is the number of times success occurred compared to the total number of trials.\n",
    "\n",
    "Odds are the number of times success occurred compared to the number of times failure occurred.\n",
    "\n",
    "For example, to predict the likelihood of accidents at a particular intersection, each car that goes through the intersection is considered a trial. Each trial has one of two outcomes: accident or safe passage. If the outcome we’re most interested in modeling is an accident, that is a success (no matter how morbid it sounds).\n",
    "\n",
    "**Probability(success)** = number of successes/total number of trials\n",
    "\n",
    "**Odds(success)** = number of successes/number of failures\n",
    "\n",
    "Odds are often written as:\n",
    "\n",
    "    `Number of successes:1 failure`\n",
    "\n",
    "    which is read as the number of successes for every 1 failure. But often the :1 is dropped.\n",
    "\n",
    "You will see a lot of researchers get stuck when learning logistic regression because they are not used to thinking of likelihood on an odds scale.\n",
    "\n",
    "- Equal odds are 1. 1 success for every 1 failure. 1:1\n",
    "- Equal probabilities are .5. 1 success for every 2 trials.\n",
    "\n",
    "\n",
    "- Odds can range from 0 to infinity. \n",
    "- Odds greater than 1 indicates success is more likely than failure. \n",
    "- Odds less than 1 indicates failure is more likely than success.\n",
    "\n",
    "\n",
    "- Probability can range from 0 to 1. \n",
    "- Probability greater than .5 indicates success is more likely than failure. \n",
    "- Probability less than .5 indicates failure is more likely than success.\n",
    "\n",
    "\n",
    "#### THE EXAMPLE: \n",
    "\n",
    "In the last month, data from a particular intersection indicate that of the 1,354 cars that drove through it, 72 got into an accident.\n",
    "\n",
    "- 72 Successes = Accident\n",
    "- 1282 Failures = Safe Passage (1,354 – 72)\n",
    "\n",
    "\n",
    "- Failures = Total – Successes\n",
    "\n",
    "\n",
    "- Pr(Accident) = 72/1354 = .053\n",
    "- Pr(Safe Passage) = 1282/1354 = .947\n",
    "\n",
    "\n",
    "- Odds(Accident) = 72/1282 = .056\n",
    "- Odds(Safety) = 1282/72 = 17.87\n",
    "\n",
    "\n",
    "Now get out your calculator, because you’ll see how these relate to each other.\n",
    "\n",
    "- Odds(Accident) = Pr(Accident)/Pr(Safety) = .053/.947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$probability = \\frac {one\\ outcome} {all\\ outcomes}$$\n",
    "# $$odds = \\frac {one\\ outcome} {all\\ other\\ outcomes}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples:\n",
    "- Dice roll of 1: probability = 1/6, odds = 1/5\n",
    "- Even dice roll: probability = 3/6, odds = 3/3 = 1\n",
    "- Dice roll less than 5: probability = 4/6, odds = 4/2 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$odds = \\frac {probability} {1 - probability}$$\n",
    "# $$probability = \\frac {odds} {1 + odds}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a table of probability versus odds\n",
    "table = pd.DataFrame({'probability':[0.1, 0.2, 0.25, 0.5, 0.6, 0.8, 0.9]})\n",
    "table['odds'] = table.probability/(1 - table.probability)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is **e**? It is the base rate of growth shared by all continually growing processes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# exponential function: e^1\n",
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a **(natural) log**? It gives you the time needed to reach a certain level of growth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# time needed to grow 1 unit to 2.718 units\n",
    "np.log(2.718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is also the **inverse** of the exponential function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.log(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add log-odds to the table\n",
    "table['logodds'] = np.log(table.odds)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4: WHAT IS LOGISTIC REGRESSION?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear regression:** continuous response is modeled as a linear combination of the features:\n",
    "# $$y = \\beta_0 + \\beta_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic regression:** log-odds of a categorical response being \"true\" (1) is modeled as a linear combination of the features:\n",
    "# $$\\log \\left({p\\over 1-p}\\right) = \\beta_0 + \\beta_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is called the **logit function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability is sometimes written as pi:\n",
    "# $$\\log \\left({\\pi\\over 1-\\pi}\\right) = \\beta_0 + \\beta_1x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The equation can be rearranged into the **logistic function**:\n",
    "# $$\\pi = \\frac{e^{\\beta_0 + \\beta_1x}} {1 + e^{\\beta_0 + \\beta_1x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In other words:\n",
    "- Logistic regression outputs the **probabilities of a specific class**\n",
    "- Those probabilities can be converted into **class predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The **logistic function** has some nice properties:\n",
    "- Takes on an \"s\" shape\n",
    "- Output is bounded by 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- **Multinomial logistic regression** is used when there are more than 2 classes.\n",
    "- Coefficients are estimated using **maximum likelihood estimation**, meaning that we choose parameters that maximize the likelihood of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 5: COMPARING LOGISTIC REGRESSION WITH OTHER OTHER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of logistic regression:\n",
    "- Highly interpretable (if you remember how)\n",
    "- Model training and prediction are fast\n",
    "- No tuning is required (excluding regularization)\n",
    "- Features don't need scaling\n",
    "- Can perform well with a small number of observations\n",
    "- Outputs well-calibrated predicted probabilities\n",
    "\n",
    "### Disadvantages of logistic regression:\n",
    "- Presumes a linear relationship between the features and the log-odds of the response\n",
    "- Performance is (generally) not competitive with the best supervised learning methods\n",
    "- Sensitive to irrelevant features\n",
    "- Can't automatically learn feature interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REFERENCES\n",
    "\n",
    "1. [Explaining Logistic Regression](http://www.theanalysisfactor.com/explaining-logistic-regression/)\n",
    "2. [Why use Odds Rations](http://www.theanalysisfactor.com/why-use-odds-ratios/)\n",
    "3. [The Intuitive Guide to Exponential Functions & e](https://betterexplained.com/articles/an-intuitive-guide-to-exponential-functions-e/)\n",
    "4. [Demystifying the Natural Logarithm](https://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
