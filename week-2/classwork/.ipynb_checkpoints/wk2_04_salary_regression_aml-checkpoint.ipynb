{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END-TO-END MODEL DEVELOPMENT, DEPLOYMENT TO CONSUMPTION\n",
    "\n",
    "We will build and deploy a machine model to predict the salary from the Stackoverflow dataset. By the end of this, you will be able to invoke a RESTful web service to get the predictions.\n",
    "\n",
    "Since the objective to demonstrate the workflow, we will use a simple two-column dataset with years of experience and salary for the experiment. For the details on the dataset, refer to my previous article on linear regression.\n",
    "\n",
    "## Prerequisites\n",
    "1. Basic knowledge of Python and Scikit-learn\n",
    "2. Active Microsoft Azure Subscription\n",
    "3. Anaconda or Miniconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Development Environment\n",
    "\n",
    "Configure a virtual environment with the Azure ML SDK. Run the below commands to install the Python SDK, and launching a Jupyter Notebook. Start a new Python 3 kernel from Jupyter.\n",
    "\n",
    "~~~\n",
    "$ conda create -n aml -y Python=3.6\n",
    " \n",
    "$ conda activate aml\n",
    " \n",
    "$ conda install nb_conda\n",
    " \n",
    "$ pip install azureml-sdk[notebooks]\n",
    " \n",
    "$ jupyter notebook\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Azure ML Environment\n",
    "\n",
    "Let’s start by importing all the required Python modules, which include standard Scikit-learn modules and the Azure ML modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.externals import joblib\n",
    " \n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create an Azure ML Workspace that acts as the logical boundary for our experiment. A Workspace creates a Storage Account for storing the dataset, a Key Vault for secrets, a Container Registry for maintaining the image repositories, and Application Insights for logging the metrics.\n",
    "\n",
    "Don’t forget to replace the placeholder with your subscription id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few minutes, we will see the resources created within the Workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new workspace in future using the code below\n",
    "#ws = Workspace.create(name='exml01-student99',\n",
    "#                      subscription_id='', \n",
    "#                      resource_group='eml-training',\n",
    "#                      create_resource_group=True,\n",
    "#                      location='southcentralus'\n",
    "#                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found workspace eml01-student99 at location southcentralus\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "\n",
    "cli_auth = AzureCliAuthentication()\n",
    "\n",
    "ws = Workspace(subscription_id=\"3c3bb71f-3a4c-436f-9e0a-7407d75a82fa\",\n",
    "               resource_group=\"eml-training\",\n",
    "               workspace_name=\"eml01-student99\",\n",
    "               auth=cli_auth)\n",
    "\n",
    "print(\"Found workspace {} at location {}\".format(ws.name, ws.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an Experiment to start logging the metrics. Since we don’t have many parameters to log, we are capturing the start time of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(workspace=ws, name='salexp')\n",
    "run = exp.start_logging()                   \n",
    "run.log(\"Experiment start time\", str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing the Scikit-learn ML Model\n",
    "\n",
    "We will now proceed to train and test the model through Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sal = pd.read_csv('../data/sal.csv',header=0, index_col=None)\n",
    "X = sal[['x']]\n",
    "y = sal['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=10)\n",
    " \n",
    "lm = LinearRegression()\n",
    "lm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model will be serialized as a pickle file in the outputs directory. Azure ML automatically copies the content of the outputs directory to the cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/sal_model.pkl']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'outputs/sal_model.pkl'\n",
    "joblib.dump(lm, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s complete the experiment by logging the slope, intercept, and the end time of the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log('Intercept :', lm.intercept_)\n",
    "run.log('Slope :', lm.coef_[0])\n",
    " \n",
    "run.log(\"Experiment end time\", str(datetime.datetime.now()))\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can track the metrics and the execution time from the Azure Dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering and Serving the Trained Model\n",
    "\n",
    "Each time we freeze the model, it can be registered with Azure ML with a unique version. This gives us the ability to easily switch between different models when serving.\n",
    "\n",
    "Let’s register the salary model from the above training job by pointing the SDK to the location of the PKL file. We are also adding some additional metadata to the model in the form of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model sal_model\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(model_path = \"outputs/sal_model.pkl\",\n",
    "                       model_name = \"sal_model\",\n",
    "                       tags = {\"key\": \"1\"},\n",
    "                       description = \"Salary Prediction\",\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the Models section of the Workspace to ensure that our model is registered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s time for us to package and deploy the model as a container image which will be exposed as a web service.\n",
    "\n",
    "For the container image to get created, we need to tell Azure ML about the environment needed by the model. We will then pass a Python script that includes code to predict the values based on an inbound data point.\n",
    "\n",
    "Azure ML API provides handy methods for both. Let’s first create the environment file, salenv.yaml, which tells the runtime to include Scikit-learn in the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\n",
      "\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\n",
      "\n",
      "\n",
      "# Details about the Conda environment file format:\n",
      "\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
      "\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\n",
      "\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\n",
      "\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "    # Required packages for AzureML execution, history, and data preparation.\n",
      "\n",
      "  - azureml-defaults\n",
      "- scikit-learn\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salenv = CondaDependencies()\n",
    "salenv.add_conda_package(\"scikit-learn\")\n",
    " \n",
    "with open(\"salenv.yml\",\"w\") as f:\n",
    "    f.write(salenv.serialize_to_string())\n",
    "\n",
    "with open(\"salenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below snippet, when executed from the Jupyter Notebook, creates a file called score.py that contains the inference logic for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    " \n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    " \n",
    "from azureml.core.model import Model\n",
    " \n",
    "def init():\n",
    "    global model\n",
    "    # retrieve the path to the model file using the model name\n",
    "    model_path = Model.get_model_path('sal_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    y_hat = model.predict(data)\n",
    "    return json.dumps(y_hat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s connect the dots by passing the inference file and environment configuration to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.99 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n",
    "                                                  runtime=\"python\", \n",
    "                                                  conda_file=\"salenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This eventually results in the creation of a container image which shows up in the Images section of the Workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set to create the deployment configuration that defines the target environment and launching it as web service hosted in Azure Container Instance as a single-vm container. We may also choose AKS or an IoT Edge environment as the deployment target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating image\n",
      "Image creation operation finished for image salary-svc02:1, operation \"Succeeded\"\n",
      "Creating service\n",
      "Running......................\n",
      "SucceededACI service creation operation finished, operation \"Succeeded\"\n"
     ]
    }
   ],
   "source": [
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"Salary\",  \"method\" : \"sklearn\"}, \n",
    "                                               description='Predict Stackoverflow Salary')\n",
    " \n",
    "service = Webservice.deploy_from_model(workspace=ws,\n",
    "                                       name='salary-svc02',\n",
    "                                       deployment_config=aciconfig,\n",
    "                                       models=[model],\n",
    "                                       image_config=image_config)\n",
    " \n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Azure Resource Group now has an Azure Container Instance running the inference for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the URL of the inference service from the below method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://52.180.88.202:80/score\n"
     ]
    }
   ],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s go ahead and invoke with the web service programatically. We can do this from the same Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"[185924.7967479675]\"\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL for the web service\n",
    "scoring_uri = service.scoring_uri\n",
    "\n",
    "# If the service is authenticated, set the key\n",
    "#key = '<your key>'\n",
    "\n",
    "# Two sets of data to score, so we get two results back\n",
    "data = { \"data\": [[45]] }\n",
    "\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "\n",
    "# Set the content type\n",
    "headers = { 'Content-Type':'application/json' }\n",
    "\n",
    "# If authentication is enabled, set the authorization header\n",
    "#headers['Authorization']=f'Bearer {key}'\n",
    "\n",
    "# Make the request and display the response\n",
    "resp = requests.post(scoring_uri, input_data, headers = headers)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniqueness of this approach is that we could perform all the tasks from a Python kernel running inside the Jupyter Notebook. Developers can do everything it takes to train and deploy ML models from code. This is the real value of using an ML PaaS like Azure ML Service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
