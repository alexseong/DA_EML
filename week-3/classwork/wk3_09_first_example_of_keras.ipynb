{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTRODUCTION TO KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "\n",
    "# a single layer with 12 artificial neurons that expects 8 input variables (also known as features):\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron can be initialized with specific weights. Keras provides a few choices, the most common of which are listed as follows:\n",
    "\n",
    "- `random_uniform`: Weights are initialized to uniformly random small values in (-0.05, 0.05). In other words, any value within the given interval is equally likely to be drawn.\n",
    "- `random_normal`: Weights are initialized according to a Gaussian, with a zero mean and small standard deviation of 0.05. For those of you who are not familiar with a Gaussian, think about a symmetric bell curve shape.\n",
    "- `zero`: All weights are initialized to zero.\n",
    "\n",
    "A full list is available at https://keras.io/initializations/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "### The first example of a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/B06258_01_02.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net is dense, meaning that each neuron in a layer is connected to all neurons located in the previous layer and to all the neurons in the following layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/B06258_01_07.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras supports a number of activation functions, and a full list is available at https://keras.io/activations/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "np.random.seed(1671) # for reproducibility\n",
    "\n",
    "# network and training\n",
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10 # number of outputs = number of digits\n",
    "OPTIMIZER = SGD() # SGD optimizer explained later \n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data: shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in 60000 x 784\n",
    "RESHAPED = 784\n",
    "\n",
    "#\n",
    "X_train = X_train.reshape(60000, RESHAPED)\n",
    "X_test = X_test.reshape(10000, RESHAPED)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize\n",
    "#\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input layer has a neuron associated with each pixel in the image for a total of 28 x 28 = 784 neurons, one for each pixel in the MNIST images.\n",
    "\n",
    "Typically, the values associated with each pixel are normalized in the range [0, 1] (which means that the intensity of each pixel is divided by 255, the maximum intensity value). The output is 10 classes, one for each digit.\n",
    "\n",
    "The final layer is a single neuron with activation function softmax, which is a generalization of the sigmoid function. Softmax squashes a k-dimensional vector of arbitrary real values into a k-dimensional vector of real values in the range (0, 1). In our case, it aggregates 10 answers provided by the previous layer with 10 neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 outputs\n",
    "# final stage is softmax\n",
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we define the model, we have to compile it so that it can be executed by the Keras backend (either Theano or TensorFlow). There are a few choices to be made during compilation:\n",
    "\n",
    "- We need to select the optimizer that is the specific algorithm used to update weights while we train our model\n",
    "- We need to select the objective function that is used by the optimizer to navigate the space of weights (frequently, objective functions are called loss function, and the process of optimization is defined as a process of loss minimization)\n",
    "- We need to evaluate the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some common choices for the objective function (a complete list of Keras objective functions is at https://keras.io/objectives/) are as follows:\n",
    "* MSE: This is the mean squared error between the predictions and the true values.\n",
    "* Binary cross-entropy: This is the binary logarithmic loss. \n",
    "* Categorical cross-entropy: This is the multiclass logarithmic loss.\n",
    "\n",
    "Some common choices for metrics (a complete list of Keras metrics is at https://keras.io/metrics/) are as follows:\n",
    "\n",
    "- Accuracy: This is the proportion of correct predictions with respect to the targets\n",
    "- Precision: This denotes how many selected items are relevant for a multilabel classification\n",
    "- Recall: This denotes how many selected items are relevant for a multilabel classification\n",
    "\n",
    "Metrics are similar to objective functions, with the only difference that they are not used for training a model but only for evaluating a model. Compiling a model in Keras is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbCallBack = TensorBoard(log_dir='./log', histogram_freq=1,\n",
    "                         write_graph=True,\n",
    "                         write_grads=True,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is compiled, it can be then trained with the `fit()` function, which specifies a few parameters:\n",
    "\n",
    "- `epochs`: This is the number of times the model is exposed to the training set. At each iteration, the optimizer tries to adjust the weights so that the objective function is minimized.\n",
    "- `batch_size`: This is the number of training instances observed before the optimizer performs a weight update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a model in Keras is very simple. Suppose we want to iterate for NB_EPOCH steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch Tensorboard on the command line to watch the model fitting process in progress\n",
    "~~~\n",
    "tensorboard --logdir ./log --host 0.0.0.0 --port 6006\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, \n",
    "                    Y_train,\n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, \n",
    "                    validation_split=VALIDATION_SPLIT,\n",
    "                    callbacks=[tbCallBack])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reserved part of the training set for validation. The key idea is that we reserve a part of the training data for measuring the performance on the validation while training. This is a good practice to follow for any machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can evaluate it on the test set that contains new unseen examples. In this way, we can get the minimal value reached by the objective function and best value reached by the evaluation metric.\n",
    "\n",
    "Note that the training set and the test set are, of course, rigorously separated. There is no point in evaluating a model on an example that has already been used for training. Learning is essentially a process intended to generalize unseen observations and not to memorize what is already known:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"Test score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the net architecture is dumped, and we can see the different types of layers used, their output shape, how many parameters they need to optimize, and how they are connected. Then, the network is trained on 48,000 samples, and 12,000 are reserved for validation. \n",
    "\n",
    "Once the neural model is built, it is then tested on 10,000 samples. As you can see, Keras is internally using TensorFlow as a backend system for computation. We can notice that the program runs for 200 iterations, and each time, the accuracy improves. When the training ends, we test our model on the test set and achieve about 92.36% accuracy on training, 92.27% on validation, and 92.22% on the test.\n",
    "\n",
    "This means that a bit less than one handwritten character out of ten is not correctly recognized. We can certainly do better than that. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just defined your first neural network in Keras."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
